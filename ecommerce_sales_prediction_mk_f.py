# -*- coding: utf-8 -*-
"""Ecommerce sales prediction_MK_F.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XdTgYsITdSztSaoLfJceIeem-JX13Ph5

With this project I am to build a predictive sales model. The dataset includes transnational data for all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered online retail selling unique all-occasion gifts.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

plt.style.use('fivethirtyeight')
# %matplotlib inline

df = pd.read_csv('/content/data.csv', encoding='latin')
df.head()

"""**Exploratory data analysis**"""

df.describe()

df.dtypes

#Convert the invoice date to date format and set it as index
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df = df.set_index('InvoiceDate')

#Indentify columns missing data for further data cleaning
df.isnull().sum()

"""CustomerID has a lot of missing values(null values) and we cannot predict this data so we will drop it."""

df=df.drop(columns=['CustomerID'])
df.head()

#Review values including null data in description
df[df['Description'].isnull()].head()

df.head()

#Replace the values to remove nulls, no columns include null values now
df['Description'] = df['Description'].fillna('UNKNOWN ITEM')
df.isnull().sum()

df['Description'].value_counts().head()

#Plot bestselling items
item_counts = df['Description'].value_counts().sort_values(ascending=False).iloc[0:10]
plt.figure(figsize=(10,5))
sns.barplot(item_counts.index, item_counts.values, palette=sns.cubehelix_palette(5))
plt.ylabel("Units")
plt.title("Top selling items");
plt.xticks(rotation=90);

#Plot most selling stock codes
stock = df['StockCode'].value_counts().sort_values(ascending=False).iloc[0:10]
plt.figure(figsize=(18,6))
sns.barplot(stock.index, stock.values, palette=sns.color_palette("GnBu_d"))
plt.ylabel("Counts")
plt.title("Most used stock codes");
plt.xticks(rotation=90);

#Review why the quantity was negative (most likely returns), all the invoices seem to start with 'C'
df[df['Quantity'] < 0].head()

df = df[~df['InvoiceNo'].str.startswith('C')]
df.head()

df.describe()

#We are interested in sales not returns, therefore, we will remove negative quantity and unit price
df = df[df['Quantity'] > 0]
df = df[df['UnitPrice'] > 0]
df.describe()

"""As we have removed null and negative values, the data seems to be more reliable. We will create a calculate field 'Sales' (quantity*unit price) which we will aim to predict with our model"""

df['Sales'] = df['Quantity'] * df['UnitPrice']
df.head()

sns.heatmap(df.corr(), annot=True)

plt.figure(figsize=(10,5))
sns.countplot(df['Country'])
plt.xticks(rotation=90)
#UK is the country with the largest amount of orders

#Plot the sales by country - NL appears to have the highest sales value
plt.figure(figsize=(10,5))
sns.barplot(x='Country', y='Sales',data=df)
plt.xticks(rotation=90)

#Review sales distribution and remove outliers
plt.figure(figsize=(10,5))
plt.scatter(x=df.index, y=df['Sales'])

df=df[df['Sales']<25000]
plt.scatter(x=df.index, y=df['Sales'])
plt.xticks(rotation=90)

df.describe()

#As most of the sales are under 17.7(75%th percentile), let's plot the distirbution
plt.figure(figsize=(10,5))
sns.distplot(df[df['Sales']<18]['Sales'], kde=True, bins=10, color='blue')

plt.figure(figsize=(10,5))
sns.distplot(df[df['Quantity']<10]['Quantity'], kde=True, bins=10, color='blue')

plt.figure(figsize=(10,5))
sns.distplot(df[df['UnitPrice']<20]['UnitPrice'], kde=True, bins=10, color='blue')

#define outliers for units price more precisely as it is not quite visible on the graph
b=plt.boxplot(df['UnitPrice'])
[item.get_ydata() for item in b['whiskers']]

"""**Analysing sales trend**"""

df_t = df[['Sales']]
df_t.head()

plt.figure(figsize=(10,6))
df_resample = df_t.resample('w').sum()
df_resample.plot()

df_d = df[df['UnitPrice'] < 8.33]
df_d.describe()

df_join = df_d.groupby('InvoiceNo')[['Quantity']].sum()

df_join = df_join.reset_index()
df_join.head()

df_d['InvoiceDate'] = df_d.index
df_d = df_d.merge(df_join, how='left', on='InvoiceNo')
df_d = df_d.rename(columns={'Quantity_x' : 'Quantity', 'Quantity_y' : 'QuantityInv'})
df_d.tail(15)

df_d.describe()

df_d['InvoiceDate'] = pd.to_datetime(df_d['InvoiceDate'])
df_d.dtypes

#df_d['Year'] = df_d['InvoiceDate'].dt.year
df_d['Month'] = df_d['InvoiceDate'].dt.month
#df_d['Week'] = df_d['InvoiceDate'].dt.week
#df_d['Day'] = df_d['InvoiceDate'].dt.day
#df_d['WeekDay'] = df_d['InvoiceDate'].dt.dayofweek
df_d.head()

#Since UK represents the largest amount of sales let's focus on the UK data
df_uk = df_d[df_d['Country'] == 'United Kingdom']
df_uk.head()

df_uk = df_uk[['Sales', 'QuantityInv','Quantity','UnitPrice', 'Month']]
df_uk.head()

from sklearn.preprocessing import scale
df_uk['QuantityInv'] = scale(df_uk['QuantityInv'])

y = df_uk['Sales']
X = df_uk.drop(columns=['Sales'])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=0)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import confusion_matrix
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.intercept_)
print(regressor.coef_)
y_pred = regressor.predict(X_test)

df_result = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df_result

regressor.score(X_test, y_test)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score, r2_score

model=RandomForestRegressor(oob_score=True,n_jobs=1, random_state=7, max_features='auto',min_samples_leaf=4)
model.fit(X_train,y_train)

pred=model.predict(X_test)

r2_score(pred,y_test)